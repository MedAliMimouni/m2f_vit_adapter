# ğŸ‹ï¸ Training Guide

> How to train DINOv3 + Mask2Former on LoveDA

## ğŸš€ Quick Start

### 1. Install dependencies
```bash
pip install -r requirements_hydra.txt
```

### 2. Set your HuggingFace token
```bash
source env.sh
# OR
export HF_TOKEN="your_token_here"
```

### 3. Train with defaults (720Ã—720, 50 epochs)
```bash
python train_hydra.py
```

That's it! ğŸ‰

---

## ğŸ“‹ Usage Examples

### Quick test (1 epoch, for debugging)
```bash
python train_hydra.py training=quick_test
```

### High-resolution training (1024Ã—1024)
```bash
python train_hydra.py --config-name=config_1024
```

### Custom hyperparameters
```bash
python train_hydra.py \
  training.max_epochs=100 \
  training.learning_rate=1e-4 \
  data.batch_size=4
```

### Lower batch size (if GPU OOM)
```bash
python train_hydra.py data.batch_size=2
```

### View the full resolved config (dry run)
```bash
python train_hydra.py --cfg job
```

---

## ğŸ”„ Training Pipeline

### What happens when you run `train_hydra.py`:

```
1. Hydra loads config from conf/
       â”‚
2. Creates run directory: runs/{timestamp}_{image_size}/
       â”‚
3. create_dinov3_mask2former() builds the model:
   â”œâ”€â”€ DINOv3-ViT-L/16 backbone (frozen)
   â”œâ”€â”€ DINOv3_Adapter (trainable)
   â””â”€â”€ Mask2Former head (trainable)
       â”‚
4. create_dataloaders() loads LoveDA:
   â”œâ”€â”€ Train split (Rural + Urban)
   â””â”€â”€ Val split (Rural + Urban)
       â”‚
5. PyTorch Lightning Trainer runs:
   â”œâ”€â”€ Training loop (forward â†’ loss â†’ backward â†’ step)
   â”œâ”€â”€ Validation every epoch (mIoU computation)
   â”œâ”€â”€ Checkpoint saving (top 3 by val_mean_iou_no_bg)
   â””â”€â”€ LR scheduling (ReduceLROnPlateau)
       â”‚
6. Outputs saved to:
   â”œâ”€â”€ runs/{timestamp}/checkpoints/*.ckpt
   â”œâ”€â”€ runs/{timestamp}/training_results.json
   â””â”€â”€ logs/ (TensorBoard + CSV)
```

---

## âš™ï¸ Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `training.max_epochs` | 50 | Total training epochs |
| `training.learning_rate` | 5e-5 | Initial learning rate |
| `training.optimizer.name` | AdamW | Optimizer |
| `training.optimizer.weight_decay` | 0.01 | Weight decay |
| `training.scheduler.name` | ReduceLROnPlateau | LR scheduler |
| `training.scheduler.factor` | 0.5 | LR reduction factor |
| `training.scheduler.patience` | 5 | Epochs before LR reduction |
| `data.batch_size` | 8 | Batch size |
| `data.image_size` | 720 | Input resolution |
| `data.num_workers` | 4 | DataLoader workers |

---

## ğŸ“Š Metrics Tracked

### During training:
- `train_loss` â€” logged every 10 steps

### During validation (every epoch):
- `val_mean_iou` â€” mIoU over **all 7 classes** (incl. background)
- `val_mean_iou_no_bg` â€” mIoU over **6 semantic classes only** â­ (primary metric)

### Model selection:
- Checkpoints saved based on **`val_mean_iou_no_bg`** (higher = better)
- Top 3 checkpoints kept

---

## ğŸ’¾ Output Structure

After training, your run directory looks like:
```
runs/2026-02-12_14-30-00_720x720/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ dinov3-mask2former-loveda-epoch=25-val_mean_iou_no_bg=0.35.ckpt
â”‚   â”œâ”€â”€ dinov3-mask2former-loveda-epoch=40-val_mean_iou_no_bg=0.38.ckpt
â”‚   â””â”€â”€ dinov3-mask2former-loveda-epoch=48-val_mean_iou_no_bg=0.40.ckpt
â”œâ”€â”€ training_results.json
â””â”€â”€ config.yaml  (saved Hydra config)
```

---

## ğŸ“Š Evaluation

### Run evaluation on a checkpoint
```bash
python evaluate_hydra.py \
  checkpoint_path=runs/2026-02-12_14-30-00_720x720/checkpoints/best.ckpt
```

### What it outputs:
- Console: mIoU (all classes), mIoU (semantic only), performance assessment
- File: `evaluation_results.json` with full metrics

---

## ğŸ§Š What's Frozen vs. Trainable

| Component | Trainable? | Why? |
|-----------|-----------|------|
| DINOv3-ViT-L/16 backbone | â„ï¸ No | Preserves pretrained representations |
| Spatial Prior Module | âœ… Yes | Learns spatial features from image |
| Interaction Blocks | âœ… Yes | Learns to fuse ViT + spatial features |
| Mask2Former head | âœ… Yes | Learns segmentation task |

---

## ğŸ› Troubleshooting

### GPU Out of Memory
```bash
# Reduce batch size
python train_hydra.py data.batch_size=2

# Or use gradient accumulation
python train_hydra.py training.accumulate_grad_batches=4 data.batch_size=2
# Effective batch = 2 Ã— 4 = 8
```

### Slow training
```bash
# Increase workers
python train_hydra.py data.num_workers=8
```

### Need HF token
```bash
source env.sh  # Sets HF_TOKEN environment variable
```

### View TensorBoard logs
```bash
tensorboard --logdir logs/
```
