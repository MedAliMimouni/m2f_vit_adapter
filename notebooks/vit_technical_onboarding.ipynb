{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) Technical Deep Dive\n",
    "## Interactive Onboarding Guide for DINOv3 + ViT-Adapter + Mask2Former\n",
    "\n",
    "---\n",
    "\n",
    "Welcome! This notebook serves as a comprehensive technical guide to our Vision Transformer-based semantic segmentation pipeline. You'll learn:\n",
    "\n",
    "1. **ViT Fundamentals & Attention Visualization** - Understanding how ViT processes images and visualizing attention\n",
    "2. **Multi-Scale Input Handling** - How positional encodings are interpolated for different resolutions\n",
    "3. **ViT-Adapter Architecture** - Bridging ViT to dense prediction tasks\n",
    "4. **MS-Deformable Attention** - Efficient multi-scale attention mechanism\n",
    "5. **Mask2Former Framework** - Universal segmentation architecture\n",
    "\n",
    "**Prerequisites:** Familiarity with CNNs, attention mechanisms, and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Vision Transformer (ViT) Fundamentals & Attention Visualization\n",
    "\n",
    "## 1.1 How ViT Works: From Pixels to Patches to Predictions\n",
    "\n",
    "Unlike CNNs that use local convolution kernels, ViT processes images as sequences of patches using self-attention:\n",
    "\n",
    "```\n",
    "Input Image (H x W x 3)\n",
    "      |\n",
    "      v\n",
    "+------------------+\n",
    "| Patch Embedding  |  Split image into P×P patches, flatten, project to D dimensions\n",
    "+------------------+\n",
    "      |\n",
    "      v\n",
    "+------------------+\n",
    "| + Positional     |  Add learnable position embeddings to each patch\n",
    "|   Embeddings     |\n",
    "+------------------+\n",
    "      |\n",
    "      v\n",
    "+------------------+\n",
    "| [CLS] + Patches  |  Prepend a learnable [CLS] token\n",
    "+------------------+\n",
    "      |\n",
    "      v\n",
    "+------------------+\n",
    "| Transformer      |  L layers of Multi-Head Self-Attention + MLP\n",
    "| Encoder Blocks   |\n",
    "+------------------+\n",
    "      |\n",
    "      v\n",
    "Output Features (N+1 tokens × D)\n",
    "```\n",
    "\n",
    "### Key Equations:\n",
    "\n",
    "**Patch Embedding:**\n",
    "$$z_0 = [x_{\\text{cls}}; x_1^{\\text{patch}}E; x_2^{\\text{patch}}E; ...; x_N^{\\text{patch}}E] + E_{\\text{pos}}$$\n",
    "\n",
    "where:\n",
    "- $E \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$ is the patch projection matrix\n",
    "- $E_{\\text{pos}} \\in \\mathbb{R}^{(N+1) \\times D}$ are learnable position embeddings\n",
    "- $N = \\frac{H \\times W}{P^2}$ is the number of patches\n",
    "\n",
    "**Self-Attention:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading a Pre-trained ViT Model\n",
    "\n",
    "We'll use `timm` (PyTorch Image Models) to load a pre-trained ViT and explore its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "# Load a pre-trained ViT model\n",
    "model_name = 'vit_base_patch16_224'\n",
    "vit_model = timm.create_model(model_name, pretrained=True)\n",
    "vit_model.eval()\n",
    "vit_model.to(device)\n",
    "\n",
    "# Inspect architecture\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Patch size: {vit_model.patch_embed.patch_size}\")\n",
    "print(f\"Embed dimension: {vit_model.embed_dim}\")\n",
    "print(f\"Number of transformer blocks: {len(vit_model.blocks)}\")\n",
    "print(f\"Number of attention heads: {vit_model.blocks[0].attn.num_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare a sample image\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Using a sample satellite image URL (or use your own local image)\n",
    "sample_image_path = \"sample_image.jpg\"\n",
    "\n",
    "# Try to use an existing image from the dataset, or download a sample\n",
    "try:\n",
    "    # Check if we have local test images\n",
    "    local_paths = [\n",
    "        os.path.join(PROJECT_ROOT, 'data', 'loveDA', 'Test', 'Rural', 'images_png', '0.png'),\n",
    "        os.path.join(PROJECT_ROOT, 'data', 'loveDA', 'Val', 'Rural', 'images_png', '0.png'),\n",
    "    ]\n",
    "    for path in local_paths:\n",
    "        if os.path.exists(path):\n",
    "            sample_image_path = path\n",
    "            break\n",
    "    else:\n",
    "        # Download a sample image\n",
    "        url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\"\n",
    "        urlretrieve(url, sample_image_path)\n",
    "except Exception as e:\n",
    "    print(f\"Using placeholder image: {e}\")\n",
    "    # Create a synthetic image\n",
    "    img = Image.new('RGB', (224, 224), color='blue')\n",
    "    img.save(sample_image_path)\n",
    "\n",
    "# Load and display the image\n",
    "img = Image.open(sample_image_path).convert('RGB')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title('Sample Input Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image size: {img.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image for ViT\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "print(f\"Input tensor shape: {img_tensor.shape}\")  # (1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Understanding Patch Tokenization\n",
    "\n",
    "Let's visualize how the image is divided into patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_size=16):\n",
    "    \"\"\"Visualize how an image is divided into patches.\"\"\"\n",
    "    img_np = np.array(image.resize((224, 224)))\n",
    "    H, W = img_np.shape[:2]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(img_np)\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(0, H + 1, patch_size):\n",
    "        ax.axhline(i, color='red', linewidth=0.5)\n",
    "    for j in range(0, W + 1, patch_size):\n",
    "        ax.axvline(j, color='red', linewidth=0.5)\n",
    "    \n",
    "    # Number some patches\n",
    "    patch_idx = 0\n",
    "    for i in range(0, H, patch_size):\n",
    "        for j in range(0, W, patch_size):\n",
    "            if patch_idx < 10:  # Only label first 10\n",
    "                ax.text(j + patch_size//2, i + patch_size//2, str(patch_idx), \n",
    "                       ha='center', va='center', color='white', fontsize=8,\n",
    "                       bbox=dict(boxstyle='round', facecolor='red', alpha=0.7))\n",
    "            patch_idx += 1\n",
    "    \n",
    "    ax.set_title(f'Image divided into {patch_idx} patches ({H//patch_size}x{W//patch_size} grid)\\n'\n",
    "                 f'Each patch: {patch_size}x{patch_size} pixels')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return patch_idx\n",
    "\n",
    "num_patches = visualize_patches(img, patch_size=16)\n",
    "print(f\"\\nTotal patches: {num_patches}\")\n",
    "print(f\"With [CLS] token: {num_patches + 1} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Attention Map Visualization\n",
    "\n",
    "### 1.4.1 Extracting Raw Attention Weights\n",
    "\n",
    "We'll hook into the attention layers to capture attention weights during forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionExtractor:\n",
    "    \"\"\"Hook-based attention extraction for ViT models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.attentions = []\n",
    "        self.hooks = []\n",
    "        \n",
    "    def _attention_hook(self, module, input, output):\n",
    "        \"\"\"Hook to capture attention weights.\"\"\"\n",
    "        # For timm ViT, attention output is (attn_output, attn_weights)\n",
    "        # but we need to modify the forward to return weights\n",
    "        pass\n",
    "    \n",
    "    def get_attention_maps(self, x):\n",
    "        \"\"\"Extract attention maps from all layers.\"\"\"\n",
    "        self.attentions = []\n",
    "        \n",
    "        # We'll manually compute attention for visualization\n",
    "        # First, get patch embeddings\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.model.patch_embed(x)\n",
    "        \n",
    "        # Add CLS token\n",
    "        cls_tokens = self.model.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add position embedding\n",
    "        x = x + self.model.pos_embed\n",
    "        x = self.model.pos_drop(x)\n",
    "        \n",
    "        # Process through transformer blocks and capture attention\n",
    "        for block in self.model.blocks:\n",
    "            # Get attention weights by computing Q, K, V manually\n",
    "            B, N, C = x.shape\n",
    "            qkv = block.attn.qkv(block.norm1(x)).reshape(\n",
    "                B, N, 3, block.attn.num_heads, C // block.attn.num_heads\n",
    "            ).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "            \n",
    "            # Compute attention weights\n",
    "            attn = (q @ k.transpose(-2, -1)) * block.attn.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            self.attentions.append(attn.detach().cpu())\n",
    "            \n",
    "            # Continue forward pass normally\n",
    "            x = x + block.drop_path1(block.ls1(block.attn(block.norm1(x))))\n",
    "            x = x + block.drop_path2(block.ls2(block.mlp(block.norm2(x))))\n",
    "        \n",
    "        return self.attentions\n",
    "\n",
    "# Alternative simpler approach using forward hooks\n",
    "def get_attention_maps_simple(model, x):\n",
    "    \"\"\"Simpler attention extraction using forward with attention output.\"\"\"\n",
    "    attentions = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # Some models return attention weights as second output\n",
    "        if isinstance(output, tuple) and len(output) > 1:\n",
    "            attentions.append(output[1])\n",
    "    \n",
    "    hooks = []\n",
    "    for block in model.blocks:\n",
    "        hook = block.attn.register_forward_hook(hook_fn)\n",
    "        hooks.append(hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    \n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    return attentions\n",
    "\n",
    "# Extract attention maps\n",
    "extractor = AttentionExtractor(vit_model)\n",
    "with torch.no_grad():\n",
    "    attention_maps = extractor.get_attention_maps(img_tensor)\n",
    "\n",
    "print(f\"Extracted attention from {len(attention_maps)} layers\")\n",
    "print(f\"Each attention map shape: {attention_maps[0].shape}\")\n",
    "print(\"  -> (batch, num_heads, num_tokens, num_tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Attention Rollout\n",
    "\n",
    "**Attention Rollout** recursively combines attention maps from all layers to trace how information flows from input patches to output tokens.\n",
    "\n",
    "**Algorithm:**\n",
    "1. For each layer, add identity matrix (residual connection): $\\tilde{A}^{(l)} = 0.5 \\cdot A^{(l)} + 0.5 \\cdot I$\n",
    "2. Normalize rows to sum to 1\n",
    "3. Multiply attention matrices: $R = \\tilde{A}^{(1)} \\cdot \\tilde{A}^{(2)} \\cdot ... \\cdot \\tilde{A}^{(L)}$\n",
    "4. Extract the [CLS] token's attention to all patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_maps, discard_ratio=0.1, head_fusion='mean'):\n",
    "    \"\"\"\n",
    "    Compute attention rollout from a list of attention maps.\n",
    "    \n",
    "    Args:\n",
    "        attention_maps: List of attention tensors (B, num_heads, N, N)\n",
    "        discard_ratio: Fraction of lowest attention values to discard\n",
    "        head_fusion: How to combine attention heads ('mean', 'max', 'min')\n",
    "    \n",
    "    Returns:\n",
    "        Attention map from [CLS] to all patches\n",
    "    \"\"\"\n",
    "    result = torch.eye(attention_maps[0].shape[-1])\n",
    "    \n",
    "    for attention in attention_maps:\n",
    "        # Fuse attention heads\n",
    "        if head_fusion == 'mean':\n",
    "            attention_fused = attention.mean(dim=1)[0]  # (N, N)\n",
    "        elif head_fusion == 'max':\n",
    "            attention_fused = attention.max(dim=1)[0][0]\n",
    "        elif head_fusion == 'min':\n",
    "            attention_fused = attention.min(dim=1)[0][0]\n",
    "        \n",
    "        # Discard lowest attention values\n",
    "        flat = attention_fused.flatten()\n",
    "        threshold = flat.kthvalue(int(flat.numel() * discard_ratio))[0]\n",
    "        attention_fused = attention_fused * (attention_fused > threshold).float()\n",
    "        \n",
    "        # Add identity for residual connection\n",
    "        I = torch.eye(attention_fused.shape[-1])\n",
    "        attention_fused = 0.5 * attention_fused + 0.5 * I\n",
    "        \n",
    "        # Normalize rows\n",
    "        attention_fused = attention_fused / attention_fused.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Accumulate\n",
    "        result = torch.matmul(attention_fused, result)\n",
    "    \n",
    "    # Get attention from [CLS] token (index 0) to all patches\n",
    "    mask = result[0, 1:]  # Exclude [CLS] token itself\n",
    "    \n",
    "    return mask\n",
    "\n",
    "# Compute attention rollout\n",
    "rollout_mask = attention_rollout(attention_maps)\n",
    "\n",
    "# Reshape to 2D grid\n",
    "num_patches_per_side = int(np.sqrt(len(rollout_mask)))\n",
    "rollout_2d = rollout_mask.reshape(num_patches_per_side, num_patches_per_side)\n",
    "\n",
    "print(f\"Rollout mask shape: {rollout_2d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(image, attention_map, title=\"Attention Map\"):\n",
    "    \"\"\"\n",
    "    Overlay attention map on original image.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    img_resized = image.resize((224, 224))\n",
    "    axes[0].imshow(img_resized)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Attention heatmap\n",
    "    attention_np = attention_map.numpy()\n",
    "    # Normalize to [0, 1]\n",
    "    attention_np = (attention_np - attention_np.min()) / (attention_np.max() - attention_np.min() + 1e-8)\n",
    "    \n",
    "    axes[1].imshow(attention_np, cmap='hot')\n",
    "    axes[1].set_title('Attention Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Overlay\n",
    "    # Resize attention to image size\n",
    "    attention_resized = Image.fromarray((attention_np * 255).astype(np.uint8))\n",
    "    attention_resized = attention_resized.resize((224, 224), resample=Image.BILINEAR)\n",
    "    attention_resized = np.array(attention_resized) / 255.0\n",
    "    \n",
    "    # Create colormap overlay\n",
    "    cmap = plt.cm.jet\n",
    "    heatmap = cmap(attention_resized)[:, :, :3]  # RGB\n",
    "    \n",
    "    # Blend with original\n",
    "    img_np = np.array(img_resized) / 255.0\n",
    "    overlay = 0.6 * img_np + 0.4 * heatmap\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    axes[2].imshow(overlay)\n",
    "    axes[2].set_title(title)\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(img, rollout_2d, \"Attention Rollout Overlay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Per-Layer Attention Analysis\n",
    "\n",
    "Let's visualize how attention patterns evolve through the network layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_layer_attentions(attention_maps, layers_to_show=[0, 3, 6, 9, 11]):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns at different layers.\n",
    "    Shows [CLS] token's attention to all patches.\n",
    "    \"\"\"\n",
    "    num_layers = len(layers_to_show)\n",
    "    fig, axes = plt.subplots(1, num_layers, figsize=(4*num_layers, 4))\n",
    "    \n",
    "    for idx, layer_idx in enumerate(layers_to_show):\n",
    "        if layer_idx >= len(attention_maps):\n",
    "            continue\n",
    "            \n",
    "        # Get attention for this layer, average across heads\n",
    "        attn = attention_maps[layer_idx].mean(dim=1)[0]  # (N, N)\n",
    "        \n",
    "        # Get [CLS] token's attention to patches (row 0, columns 1:)\n",
    "        cls_attn = attn[0, 1:].numpy()  # Exclude [CLS] attending to itself\n",
    "        \n",
    "        # Reshape to 2D\n",
    "        size = int(np.sqrt(len(cls_attn)))\n",
    "        cls_attn_2d = cls_attn.reshape(size, size)\n",
    "        \n",
    "        axes[idx].imshow(cls_attn_2d, cmap='viridis')\n",
    "        axes[idx].set_title(f'Layer {layer_idx + 1}')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle('[CLS] Token Attention to Patches Across Layers', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_layer_attentions(attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Multi-Head Attention Visualization\n",
    "\n",
    "Different attention heads learn to focus on different aspects of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_heads(attention_maps, layer_idx=-1, num_heads_to_show=6):\n",
    "    \"\"\"\n",
    "    Visualize individual attention heads from a specific layer.\n",
    "    \"\"\"\n",
    "    attn = attention_maps[layer_idx][0]  # (num_heads, N, N)\n",
    "    num_heads = min(attn.shape[0], num_heads_to_show)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_heads // 2, figsize=(3 * (num_heads // 2), 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head_idx in range(num_heads):\n",
    "        # Get [CLS] attention to patches for this head\n",
    "        head_attn = attn[head_idx, 0, 1:].numpy()\n",
    "        size = int(np.sqrt(len(head_attn)))\n",
    "        head_attn_2d = head_attn.reshape(size, size)\n",
    "        \n",
    "        axes[head_idx].imshow(head_attn_2d, cmap='magma')\n",
    "        axes[head_idx].set_title(f'Head {head_idx + 1}')\n",
    "        axes[head_idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Attention Heads from Layer {layer_idx if layer_idx >= 0 else len(attention_maps) + layer_idx + 1}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_heads(attention_maps, layer_idx=-1)  # Last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Handling Multi-Scale Inputs via Positional Encoding Interpolation\n",
    "\n",
    "## 2.1 The Fixed Position Encoding Problem\n",
    "\n",
    "### Why Standard ViT Fails on Different Resolutions\n",
    "\n",
    "ViT learns positional embeddings for a **fixed number of patches**. For example:\n",
    "- Training size: 224×224 with 16×16 patches → 14×14 = 196 patches\n",
    "- Position embedding shape: (1, 197, D) including [CLS]\n",
    "\n",
    "**Problem:** What happens if we want to use 512×512 images?\n",
    "- 512×512 with 16×16 patches → 32×32 = 1024 patches\n",
    "- We only have 196 position embeddings!\n",
    "\n",
    "```\n",
    "Training Resolution:              Inference at Higher Resolution:\n",
    "+-+-+-+-+                         +-+-+-+-+-+-+-+-+\n",
    "|0|1|2|3|   14×14 = 196 patches  |0|1|2|3|4|5|6|7|   32×32 = 1024 patches\n",
    "+-+-+-+-+   with known positions +-+-+-+-+-+-+-+-+   MISSING positions!\n",
    "|4|5|...|                        |8|9|...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Solution: 2D Bicubic Interpolation of Position Embeddings\n",
    "\n",
    "The key insight is that position embeddings have **2D spatial structure**:\n",
    "\n",
    "1. **Reshape** the flat position embeddings to a 2D grid\n",
    "2. **Interpolate** to the target resolution using bicubic interpolation\n",
    "3. **Flatten** back to 1D sequence\n",
    "\n",
    "This works because position embeddings encode **relative spatial relationships**, which should be preserved under smooth interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pos_encoding(\n",
    "    pos_embed: torch.Tensor,\n",
    "    src_size: tuple,  # (H_src, W_src) in patches\n",
    "    tgt_size: tuple,  # (H_tgt, W_tgt) in patches\n",
    "    num_extra_tokens: int = 1,  # Usually 1 for [CLS], DINOv3 uses 5 (1 CLS + 4 registers)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Interpolate positional encodings from source to target resolution.\n",
    "    \n",
    "    This is the core function used in our codebase at:\n",
    "    models/backbone/dinov3_adapter.py:387-396 (_get_pos_embed method)\n",
    "    \n",
    "    Args:\n",
    "        pos_embed: Position embeddings of shape (1, N_src + extra, D)\n",
    "        src_size: Original grid size in patches (H_src, W_src)\n",
    "        tgt_size: Target grid size in patches (H_tgt, W_tgt)\n",
    "        num_extra_tokens: Number of non-patch tokens ([CLS], registers)\n",
    "    \n",
    "    Returns:\n",
    "        Interpolated position embeddings of shape (1, N_tgt + extra, D)\n",
    "    \"\"\"\n",
    "    D = pos_embed.shape[-1]\n",
    "    H_src, W_src = src_size\n",
    "    H_tgt, W_tgt = tgt_size\n",
    "    \n",
    "    # Separate extra tokens (CLS, registers) from patch embeddings\n",
    "    extra_tokens = pos_embed[:, :num_extra_tokens, :]  # (1, extra, D)\n",
    "    patch_pos_embed = pos_embed[:, num_extra_tokens:, :]  # (1, H*W, D)\n",
    "    \n",
    "    # Reshape to 2D spatial grid: (1, H, W, D) -> (1, D, H, W)\n",
    "    patch_pos_embed = patch_pos_embed.reshape(1, H_src, W_src, D).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Bicubic interpolation to target size\n",
    "    patch_pos_embed = F.interpolate(\n",
    "        patch_pos_embed,\n",
    "        size=(H_tgt, W_tgt),\n",
    "        mode='bicubic',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    # Reshape back: (1, D, H, W) -> (1, H*W, D)\n",
    "    patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).reshape(1, H_tgt * W_tgt, D)\n",
    "    \n",
    "    # Concatenate extra tokens back\n",
    "    interpolated = torch.cat([extra_tokens, patch_pos_embed], dim=1)\n",
    "    \n",
    "    return interpolated\n",
    "\n",
    "# Example: Interpolate from 14×14 (224px) to 32×32 (512px)\n",
    "print(\"Position Embedding Interpolation Demo:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Original position embeddings (simulated)\n",
    "src_H, src_W = 14, 14  # 224×224 image with 16×16 patches\n",
    "embed_dim = 768\n",
    "\n",
    "# Create dummy position embeddings\n",
    "pos_embed_original = torch.randn(1, 1 + src_H * src_W, embed_dim)\n",
    "print(f\"Original pos_embed shape: {pos_embed_original.shape}\")\n",
    "print(f\"  - 1 CLS token + {src_H}×{src_W} = {src_H*src_W} patch tokens\")\n",
    "\n",
    "# Interpolate to larger resolution\n",
    "tgt_H, tgt_W = 32, 32  # 512×512 image\n",
    "pos_embed_interpolated = interpolate_pos_encoding(\n",
    "    pos_embed_original,\n",
    "    src_size=(src_H, src_W),\n",
    "    tgt_size=(tgt_H, tgt_W),\n",
    "    num_extra_tokens=1\n",
    ")\n",
    "print(f\"\\nInterpolated pos_embed shape: {pos_embed_interpolated.shape}\")\n",
    "print(f\"  - 1 CLS token + {tgt_H}×{tgt_W} = {tgt_H*tgt_W} patch tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pos_encoding_interpolation():\n",
    "    \"\"\"\n",
    "    Visualize the effect of position encoding interpolation.\n",
    "    \"\"\"\n",
    "    # Create position embeddings that encode position (for visualization)\n",
    "    src_H, src_W = 14, 14\n",
    "    D = 3  # Use 3 dims for RGB visualization\n",
    "    \n",
    "    # Create embeddings that encode (x, y, x*y) for visualization\n",
    "    pos = torch.zeros(1, 1 + src_H * src_W, D)\n",
    "    for i in range(src_H):\n",
    "        for j in range(src_W):\n",
    "            idx = 1 + i * src_W + j  # Skip CLS\n",
    "            pos[0, idx, 0] = i / (src_H - 1)  # Normalized row position\n",
    "            pos[0, idx, 1] = j / (src_W - 1)  # Normalized col position  \n",
    "            pos[0, idx, 2] = (i + j) / (src_H + src_W - 2)  # Diagonal\n",
    "    \n",
    "    # Interpolate to higher resolution\n",
    "    tgt_H, tgt_W = 45, 45  # 720×720 image (our training size!)\n",
    "    pos_interp = interpolate_pos_encoding(\n",
    "        pos, (src_H, src_W), (tgt_H, tgt_W), num_extra_tokens=1\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original\n",
    "    orig_vis = pos[0, 1:].reshape(src_H, src_W, 3).numpy()\n",
    "    axes[0].imshow(orig_vis)\n",
    "    axes[0].set_title(f'Original Position Encoding\\n{src_H}×{src_W} = {src_H*src_W} patches\\n(224×224 image)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Interpolated\n",
    "    interp_vis = pos_interp[0, 1:].reshape(tgt_H, tgt_W, 3).numpy()\n",
    "    axes[1].imshow(interp_vis)\n",
    "    axes[1].set_title(f'Interpolated Position Encoding\\n{tgt_H}×{tgt_W} = {tgt_H*tgt_W} patches\\n(720×720 image)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Position Encoding Interpolation via Bicubic Upsampling', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_pos_encoding_interpolation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DINOv3 Position Encoding in Our Codebase\n",
    "\n",
    "DINOv3 uses a slightly different token structure with **register tokens**:\n",
    "\n",
    "```\n",
    "Token Sequence: [CLS, patch_1, patch_2, ..., patch_N, reg_1, reg_2, reg_3, reg_4]\n",
    "```\n",
    "\n",
    "Our `DINOv3CompatibilityWrapper` handles this in `dinov3_mask2former_integration.py:85-155`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from our codebase showing how DINOv3 token structure is handled\n",
    "dinov3_token_handling = \"\"\"\n",
    "# From dinov3_mask2former_integration.py\n",
    "\n",
    "class DINOv3CompatibilityWrapper(nn.Module):\n",
    "    '''\n",
    "    Wraps HuggingFace DINOv3 to be compatible with adapter expectations.\n",
    "    \n",
    "    DINOv3 output structure:\n",
    "    - Token 0: [CLS] token\n",
    "    - Tokens 1 to N: Patch tokens (N = H*W / patch_size^2)\n",
    "    - Tokens N+1 to N+4: Register tokens (4 registers)\n",
    "    '''\n",
    "    \n",
    "    def get_intermediate_layers(self, x, n, return_class_token=True):\n",
    "        # Get features at specific layers\n",
    "        outputs = self.model(\n",
    "            x, output_hidden_states=True, return_dict=True\n",
    "        )\n",
    "        hidden_states = outputs.hidden_states[1:]  # Skip embedding\n",
    "        \n",
    "        results = []\n",
    "        for idx in n:  # n = interaction_indexes like [4, 11, 17, 23]\n",
    "            h = hidden_states[idx]\n",
    "            \n",
    "            # Extract components from token sequence\n",
    "            cls_token = h[:, 0:1, :]           # First token is CLS\n",
    "            patch_tokens = h[:, 1:-4, :]       # Middle tokens are patches\n",
    "            # register_tokens = h[:, -4:, :]  # Last 4 are registers (unused)\n",
    "            \n",
    "            results.append((patch_tokens, cls_token))\n",
    "        \n",
    "        return results\n",
    "\"\"\"\n",
    "print(dinov3_token_handling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: The ViT-Adapter Architecture\n",
    "\n",
    "## 3.1 Why Do We Need an Adapter?\n",
    "\n",
    "### The Gap Between ViT and Dense Prediction\n",
    "\n",
    "**Plain ViT outputs:**\n",
    "- Single-scale features at patch resolution (H/16, W/16)\n",
    "- Global receptive field from layer 1 (no local inductive bias)\n",
    "- 1D sequence without explicit spatial structure\n",
    "\n",
    "**Dense prediction (segmentation) needs:**\n",
    "- Multi-scale features (H/4, H/8, H/16, H/32) for FPN\n",
    "- Local features for fine boundaries\n",
    "- Spatial relationships preserved\n",
    "\n",
    "**ViT-Adapter bridges this gap!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 ViT-Adapter Architecture Overview\n",
    "\n",
    "```\n",
    "                              ViT Transformer (Frozen)\n",
    "                              ========================\n",
    "Input Image                   Layer 4    Layer 11   Layer 17   Layer 23\n",
    "    |                            |           |          |          |\n",
    "    |                            |           |          |          |\n",
    "    v                            v           v          v          v\n",
    "+--------+                   [Patch Tokens + CLS from each layer]\n",
    "| Spatial|                       |           |          |          |\n",
    "| Prior  |--c2,c3,c4-->[Interaction Blocks with Deformable Attention]\n",
    "| Module |                       |           |          |          |\n",
    "+--------+                       v           v          v          v\n",
    "    |                        [Refined Multi-Scale Features]\n",
    "    |                            |           |          |          |\n",
    "    |                            v           v          v          v\n",
    "    +------------------------> f1(H/4)    f2(H/8)   f3(H/16)  f4(H/32)\n",
    "                                 |           |          |          |\n",
    "                                 +-----------+----------+----------+\n",
    "                                             |\n",
    "                                             v\n",
    "                                    [FPN / Pixel Decoder]\n",
    "                                             |\n",
    "                                             v\n",
    "                                    [Mask2Former Head]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Spatial Prior Module (SPM)\n",
    "\n",
    "The SPM is a lightweight CNN that provides **local inductive bias** and **multi-scale spatial features**.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Input: (B, 3, H, W)\n",
    "    |\n",
    "    v\n",
    "Stem: Conv(3→64, k=3) → BN → ReLU → Conv(64→64, k=3) → BN → ReLU → MaxPool\n",
    "    |                                                              |\n",
    "    |                                                              v\n",
    "    |                                                    c1: (B, 64, H/4, W/4)\n",
    "    |                                                              |\n",
    "    v                                                              |\n",
    "Conv2: Conv(64→128, stride=2) → BN → ReLU                        |\n",
    "    |                                                              |\n",
    "    v                                                              |\n",
    "fc2: Conv(128→1024, k=1) → c2: (B, 1024, H/8, W/8) ←--------------+\n",
    "    |                                                              \n",
    "    v                                                              \n",
    "Conv3: Conv(128→256, stride=2) → BN → ReLU                        \n",
    "    |                                                              \n",
    "    v                                                              \n",
    "fc3: Conv(256→1024, k=1) → c3: (B, 1024, H/16, W/16)              \n",
    "    |                                                              \n",
    "    v                                                              \n",
    "Conv4: Conv(256→256, stride=2) → BN → ReLU                        \n",
    "    |                                                              \n",
    "    v                                                              \n",
    "fc4: Conv(256→1024, k=1) → c4: (B, 1024, H/32, W/32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified implementation of Spatial Prior Module\n",
    "class SpatialPriorModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates multi-scale spatial priors using convolutional stem.\n",
    "    \n",
    "    This provides the local inductive bias that ViT lacks.\n",
    "    Located at: models/backbone/dinov3_adapter.py:234-302\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inplanes=64, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stem: 3 conv layers + max pool → H/4 resolution\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, inplanes, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        \n",
    "        # Downsample stages\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, inplanes * 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(inplanes * 2, inplanes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(inplanes * 4, inplanes * 4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(inplanes * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Projection to embedding dimension\n",
    "        self.fc1 = nn.Conv2d(inplanes, embed_dim, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(inplanes * 2, embed_dim, kernel_size=1)\n",
    "        self.fc3 = nn.Conv2d(inplanes * 4, embed_dim, kernel_size=1)\n",
    "        self.fc4 = nn.Conv2d(inplanes * 4, embed_dim, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, 3, H, W)\n",
    "        c1 = self.stem(x)          # (B, 64, H/4, W/4)\n",
    "        c2 = self.conv2(c1)        # (B, 128, H/8, W/8)\n",
    "        c3 = self.conv3(c2)        # (B, 256, H/16, W/16)\n",
    "        c4 = self.conv4(c3)        # (B, 256, H/32, W/32)\n",
    "        \n",
    "        # Project to embedding dimension and flatten to tokens\n",
    "        c1 = self.fc1(c1)  # (B, 1024, H/4, W/4)\n",
    "        c2 = self.fc2(c2).flatten(2).transpose(1, 2)  # (B, N2, 1024)\n",
    "        c3 = self.fc3(c3).flatten(2).transpose(1, 2)  # (B, N3, 1024)\n",
    "        c4 = self.fc4(c4).flatten(2).transpose(1, 2)  # (B, N4, 1024)\n",
    "        \n",
    "        return c1, c2, c3, c4\n",
    "\n",
    "# Demo\n",
    "spm = SpatialPriorModule(inplanes=64, embed_dim=1024)\n",
    "dummy_input = torch.randn(1, 3, 720, 720)  # Our training resolution\n",
    "c1, c2, c3, c4 = spm(dummy_input)\n",
    "\n",
    "print(\"Spatial Prior Module Output Shapes:\")\n",
    "print(f\"  c1 (H/4):  {c1.shape}  - kept as spatial for final upsample\")\n",
    "print(f\"  c2 (H/8):  {c2.shape}  - {720//8}×{720//8} = {(720//8)**2} tokens\")\n",
    "print(f\"  c3 (H/16): {c3.shape}  - {720//16}×{720//16} = {(720//16)**2} tokens\")\n",
    "print(f\"  c4 (H/32): {c4.shape}  - {720//32}×{720//32} = {(720//32)**2} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Interaction Blocks (Injectors)\n",
    "\n",
    "Interaction blocks **fuse** spatial priors from SPM with semantic features from ViT using **MS-Deformable Attention**.\n",
    "\n",
    "```\n",
    "ViT Patch Tokens (single scale)    SPM Tokens (multi-scale: c2, c3, c4)\n",
    "         |                                      |\n",
    "         v                                      v\n",
    "    [Add level embed]                    [Concatenate all scales]\n",
    "         |                                      |\n",
    "         +-------------> Deformable <-----------+\n",
    "                         Cross-Attention\n",
    "                              |\n",
    "                              v\n",
    "                    [Query = ViT tokens]\n",
    "                    [Key/Value = SPM tokens]\n",
    "                              |\n",
    "                              v\n",
    "              [ViT tokens enriched with local spatial info]\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- SPM provides **local features** (edges, textures) via convolutions\n",
    "- ViT provides **global semantics** via self-attention\n",
    "- Deformable attention allows **adaptive fusion** - each ViT token queries relevant SPM locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Complete ViT-Adapter Forward Pass\n",
    "\n",
    "Here's the conceptual flow (simplified from `models/backbone/dinov3_adapter.py:408-484`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual forward pass (simplified pseudocode)\n",
    "vit_adapter_forward = \"\"\"\n",
    "def forward(self, x):\n",
    "    '''\n",
    "    ViT-Adapter forward pass.\n",
    "    Input: x (B, 3, H, W)\n",
    "    Output: dict of multi-scale features {\"1\": f1, \"2\": f2, \"3\": f3, \"4\": f4}\n",
    "    '''\n",
    "    \n",
    "    # 1. Generate spatial priors via CNN stem\n",
    "    c1, c2, c3, c4 = self.spm(x)  # Multi-scale local features\n",
    "    \n",
    "    # 2. Add learnable level embeddings to distinguish scales\n",
    "    c2 = c2 + self.level_embed[0]  # H/8 scale\n",
    "    c3 = c3 + self.level_embed[1]  # H/16 scale\n",
    "    c4 = c4 + self.level_embed[2]  # H/32 scale\n",
    "    \n",
    "    # 3. Concatenate all SPM tokens for deformable attention\n",
    "    c = torch.cat([c2, c3, c4], dim=1)  # (B, N2+N3+N4, D)\n",
    "    \n",
    "    # 4. Get ViT intermediate features at interaction points\n",
    "    # interaction_indexes = [4, 11, 17, 23] for 24-layer ViT\n",
    "    vit_features = backbone.get_intermediate_layers(\n",
    "        x, n=self.interaction_indexes, return_class_token=True\n",
    "    )\n",
    "    \n",
    "    # 5. Interaction: Fuse ViT and SPM features\n",
    "    outs = []\n",
    "    for i, interaction_block in enumerate(self.interactions):\n",
    "        vit_tokens, cls_token = vit_features[i]\n",
    "        \n",
    "        # Deformable attention: ViT queries SPM\n",
    "        vit_tokens, c, cls = interaction_block(\n",
    "            vit_tokens, c, cls_token,\n",
    "            deform_inputs1, deform_inputs2,\n",
    "            H, W\n",
    "        )\n",
    "        outs.append(vit_tokens)\n",
    "    \n",
    "    # 6. Split concatenated tokens back to scales\n",
    "    c2, c3, c4 = split_by_scale(c)\n",
    "    \n",
    "    # 7. Reshape tokens to spatial feature maps\n",
    "    c2 = c2.transpose(1,2).view(B, D, H//8, W//8)\n",
    "    c3 = c3.transpose(1,2).view(B, D, H//16, W//16)\n",
    "    c4 = c4.transpose(1,2).view(B, D, H//32, W//32)\n",
    "    c1 = upsample(c2) + c1  # High-res via upsampling\n",
    "    \n",
    "    # 8. Add ViT features to output (if enabled)\n",
    "    if self.add_vit_feature:\n",
    "        x1, x2, x3, x4 = reshape_vit_outputs(outs)\n",
    "        c1, c2, c3, c4 = c1+x1, c2+x2, c3+x3, c4+x4\n",
    "    \n",
    "    # 9. Final BatchNorm\n",
    "    f1 = self.norm1(c1)  # H/4\n",
    "    f2 = self.norm2(c2)  # H/8\n",
    "    f3 = self.norm3(c3)  # H/16\n",
    "    f4 = self.norm4(c4)  # H/32\n",
    "    \n",
    "    return {\"1\": f1, \"2\": f2, \"3\": f3, \"4\": f4}\n",
    "\"\"\"\n",
    "print(vit_adapter_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Multi-Scale Deformable Attention (MS-Deform-Attn)\n",
    "\n",
    "## 4.1 The Problem with Standard Attention for Dense Prediction\n",
    "\n",
    "**Standard Self-Attention:**\n",
    "- Complexity: $O(N^2)$ where $N$ = number of tokens\n",
    "- For a 720×720 image with 16×16 patches: $N = 45 \\times 45 = 2025$ tokens\n",
    "- Attention matrix: $2025 \\times 2025 \\approx 4M$ elements per head\n",
    "\n",
    "**For multi-scale dense prediction, it's worse:**\n",
    "- H/8: 90×90 = 8,100 tokens\n",
    "- H/16: 45×45 = 2,025 tokens  \n",
    "- H/32: 22×22 = 484 tokens\n",
    "- Total: ~10,600 tokens → $O(112M)$ attention elements!\n",
    "\n",
    "**Solution: Deformable Attention** - Only attend to a **small fixed number of sampling points** per query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Deformable Attention: Key Concepts\n",
    "\n",
    "### Reference Points\n",
    "Each query token has a **reference point** - a normalized (x, y) coordinate indicating its \"home\" position:\n",
    "\n",
    "```\n",
    "Reference Point Grid (for H/16 = 45×45 tokens):\n",
    "\n",
    "(0.0, 0.0)  (0.02, 0.0)  ...  (1.0, 0.0)\n",
    "(0.0, 0.02) (0.02, 0.02) ...  (1.0, 0.02)\n",
    "    ...          ...              ...\n",
    "(0.0, 1.0)  (0.02, 1.0)  ...  (1.0, 1.0)\n",
    "```\n",
    "\n",
    "### Sampling Offsets\n",
    "Instead of attending to ALL positions, each query **learns offsets** to sample a small number of points:\n",
    "\n",
    "$$\\text{sampling\\_location} = \\text{reference\\_point} + \\Delta p$$\n",
    "\n",
    "where $\\Delta p$ is predicted by a linear layer from the query.\n",
    "\n",
    "### Multi-Scale Sampling\n",
    "For each query, sample points at **multiple scales** (H/8, H/16, H/32):\n",
    "\n",
    "```\n",
    "Query at position (0.5, 0.5):\n",
    "\n",
    "Scale H/8:   Sample 4 points near (0.5, 0.5)\n",
    "Scale H/16:  Sample 4 points near (0.5, 0.5)\n",
    "Scale H/32:  Sample 4 points near (0.5, 0.5)\n",
    "\n",
    "Total: 3 scales × 4 points × 8 heads = 96 points vs 10,600 in full attention!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Mathematical Formulation\n",
    "\n",
    "**Multi-Scale Deformable Attention:**\n",
    "\n",
    "$$\\text{MSDeformAttn}(z_q, \\hat{p}_q, \\{x^l\\}_{l=1}^{L}) = \\sum_{m=1}^{M} W_m \\left[ \\sum_{l=1}^{L} \\sum_{k=1}^{K} A_{mlqk} \\cdot W'_m x^l(\\phi_l(\\hat{p}_q) + \\Delta p_{mlqk}) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $z_q$: Query embedding\n",
    "- $\\hat{p}_q$: Reference point (normalized coordinates)\n",
    "- $x^l$: Features at scale $l$ (out of $L$ total scales)\n",
    "- $M$: Number of attention heads\n",
    "- $K$: Number of sampling points per head per scale (typically 4)\n",
    "- $A_{mlqk}$: Attention weight (learnable, normalized via softmax)\n",
    "- $\\Delta p_{mlqk}$: Sampling offset (learnable)\n",
    "- $W_m, W'_m$: Projection matrices\n",
    "- $\\phi_l$: Scale-specific coordinate adjustment\n",
    "\n",
    "**Complexity:** $O(N \\cdot M \\cdot L \\cdot K)$ - **linear** in the number of queries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified MS-Deformable Attention Implementation\n",
    "# Full implementation at: models/utils/ms_deform_attn.py\n",
    "\n",
    "class MSDeformAttnSimplified(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Multi-Scale Deformable Attention for understanding.\n",
    "    \n",
    "    Key insight: Instead of attending to all N positions,\n",
    "    each query only samples K points per scale per head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_levels = n_levels\n",
    "        self.n_heads = n_heads\n",
    "        self.n_points = n_points\n",
    "        \n",
    "        # Predict sampling offsets: query -> (n_heads * n_levels * n_points * 2)\n",
    "        self.sampling_offsets = nn.Linear(\n",
    "            d_model, n_heads * n_levels * n_points * 2\n",
    "        )\n",
    "        \n",
    "        # Predict attention weights: query -> (n_heads * n_levels * n_points)\n",
    "        self.attention_weights = nn.Linear(\n",
    "            d_model, n_heads * n_levels * n_points\n",
    "        )\n",
    "        \n",
    "        # Value projection\n",
    "        self.value_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, reference_points, input_flatten, \n",
    "                input_spatial_shapes, input_level_start_index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (N, Len_q, d_model) - Query embeddings\n",
    "            reference_points: (N, Len_q, n_levels, 2) - Normalized (x,y) coords\n",
    "            input_flatten: (N, sum(H_l*W_l), d_model) - Flattened multi-scale features\n",
    "            input_spatial_shapes: (n_levels, 2) - [(H_0, W_0), (H_1, W_1), ...]\n",
    "            input_level_start_index: (n_levels,) - Starting index for each level\n",
    "        \n",
    "        Returns:\n",
    "            output: (N, Len_q, d_model)\n",
    "        \"\"\"\n",
    "        N, Len_q, _ = query.shape\n",
    "        N, Len_in, _ = input_flatten.shape\n",
    "        \n",
    "        # 1. Project values\n",
    "        value = self.value_proj(input_flatten)  # (N, Len_in, d_model)\n",
    "        \n",
    "        # 2. Predict sampling offsets from query\n",
    "        sampling_offsets = self.sampling_offsets(query)  # (N, Len_q, n_heads*n_levels*n_points*2)\n",
    "        sampling_offsets = sampling_offsets.view(\n",
    "            N, Len_q, self.n_heads, self.n_levels, self.n_points, 2\n",
    "        )\n",
    "        \n",
    "        # 3. Predict attention weights from query\n",
    "        attention_weights = self.attention_weights(query)  # (N, Len_q, n_heads*n_levels*n_points)\n",
    "        attention_weights = F.softmax(\n",
    "            attention_weights.view(N, Len_q, self.n_heads, self.n_levels * self.n_points),\n",
    "            dim=-1\n",
    "        ).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)\n",
    "        \n",
    "        # 4. Compute sampling locations: reference_point + offset\n",
    "        # (Actual implementation uses scale-specific normalization)\n",
    "        sampling_locations = reference_points[:, :, None, :, None, :] + sampling_offsets\n",
    "        \n",
    "        # 5. Sample values at computed locations using bilinear interpolation\n",
    "        # (This is where the CUDA kernel provides speedup)\n",
    "        # output = sample_and_aggregate(value, sampling_locations, attention_weights)\n",
    "        \n",
    "        # 6. Project output\n",
    "        # output = self.output_proj(output)\n",
    "        \n",
    "        return None  # Simplified - actual impl returns sampled output\n",
    "\n",
    "# Demo: Show complexity comparison\n",
    "print(\"Complexity Comparison (720×720 image):\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Standard attention\n",
    "N_h8 = (720 // 8) ** 2   # 8100 tokens at H/8\n",
    "N_h16 = (720 // 16) ** 2  # 2025 tokens at H/16\n",
    "N_h32 = (720 // 32) ** 2  # 506 tokens at H/32\n",
    "N_total = N_h8 + N_h16 + N_h32\n",
    "\n",
    "standard_complexity = N_total ** 2\n",
    "print(f\"Total multi-scale tokens: {N_total:,}\")\n",
    "print(f\"Standard attention: O(N²) = {standard_complexity:,} elements\")\n",
    "\n",
    "# Deformable attention\n",
    "n_heads = 16\n",
    "n_levels = 3\n",
    "n_points = 4\n",
    "deform_complexity = N_total * n_heads * n_levels * n_points\n",
    "print(f\"\\nDeformable attention: O(N × M × L × K)\")\n",
    "print(f\"  N={N_total}, M={n_heads} heads, L={n_levels} levels, K={n_points} points\")\n",
    "print(f\"  = {deform_complexity:,} elements\")\n",
    "\n",
    "print(f\"\\nSpeedup: {standard_complexity / deform_complexity:.1f}x fewer operations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_deformable_sampling():\n",
    "    \"\"\"\n",
    "    Visualize how deformable attention samples points at different scales.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    scales = [(90, 90, 'H/8'), (45, 45, 'H/16'), (22, 22, 'H/32')]\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    # Reference point (center of image)\n",
    "    ref_x, ref_y = 0.5, 0.5\n",
    "    \n",
    "    for idx, (H, W, label) in enumerate(scales):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Draw grid representing feature map\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        \n",
    "        # Grid lines\n",
    "        for i in range(H + 1):\n",
    "            ax.axhline(i / H, color='lightgray', linewidth=0.5)\n",
    "        for j in range(W + 1):\n",
    "            ax.axvline(j / W, color='lightgray', linewidth=0.5)\n",
    "        \n",
    "        # Reference point\n",
    "        ax.scatter([ref_x], [ref_y], c='black', s=200, marker='*', \n",
    "                   zorder=10, label='Reference Point')\n",
    "        \n",
    "        # Simulate 4 sampling points with learned offsets\n",
    "        np.random.seed(42 + idx)\n",
    "        for k in range(4):\n",
    "            # Random offset (in practice, these are learned)\n",
    "            offset_x = np.random.uniform(-0.15, 0.15)\n",
    "            offset_y = np.random.uniform(-0.15, 0.15)\n",
    "            sample_x = ref_x + offset_x\n",
    "            sample_y = ref_y + offset_y\n",
    "            \n",
    "            # Draw sampling point\n",
    "            ax.scatter([sample_x], [sample_y], c=colors[k], s=100, \n",
    "                      marker='o', zorder=5)\n",
    "            # Draw offset arrow\n",
    "            ax.annotate('', xy=(sample_x, sample_y), xytext=(ref_x, ref_y),\n",
    "                       arrowprops=dict(arrowstyle='->', color=colors[k], lw=2))\n",
    "        \n",
    "        ax.set_title(f'{label} Scale\\n({H}×{W} = {H*W} positions)\\n4 sampled points')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.invert_yaxis()  # Match image coordinates\n",
    "    \n",
    "    plt.suptitle('Multi-Scale Deformable Attention: Sparse Sampling per Query', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insight: Instead of attending to all ~10,600 positions,\")\n",
    "    print(\"each query samples only 4 points per scale = 12 total points!\")\n",
    "\n",
    "visualize_deformable_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: The Mask2Former Framework\n",
    "\n",
    "## 5.1 Overview: Universal Segmentation\n",
    "\n",
    "Mask2Former unifies semantic, instance, and panoptic segmentation under one architecture:\n",
    "\n",
    "```\n",
    "                        Input Image\n",
    "                             |\n",
    "                             v\n",
    "                    +----------------+\n",
    "                    | Backbone       |  ← DINOv3 + ViT-Adapter\n",
    "                    | (Multi-scale)  |    Outputs: {f1, f2, f3, f4}\n",
    "                    +----------------+\n",
    "                             |\n",
    "                             v\n",
    "                    +----------------+\n",
    "                    | Pixel Decoder  |  ← FPN-style feature fusion\n",
    "                    | (FPN + Deform) |    Outputs: High-res features\n",
    "                    +----------------+\n",
    "                             |\n",
    "                             v\n",
    "                    +----------------+\n",
    "                    | Transformer    |  ← Query-based decoding\n",
    "                    | Decoder        |    with Masked Attention\n",
    "                    +----------------+\n",
    "                         /     \\\n",
    "                        /       \\\n",
    "                       v         v\n",
    "              +------------+  +------------+\n",
    "              | Class Head |  | Mask Head  |\n",
    "              +------------+  +------------+\n",
    "                    |              |\n",
    "                    v              v\n",
    "              Class Logits    Mask Logits\n",
    "              (N×C)           (N×H×W)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Component 1: Pixel Decoder\n",
    "\n",
    "The Pixel Decoder fuses multi-scale backbone features into high-resolution feature maps.\n",
    "\n",
    "### Architecture (MSDeformAttn-based):\n",
    "\n",
    "```\n",
    "Backbone Features:     f4 (H/32)    f3 (H/16)    f2 (H/8)    f1 (H/4)\n",
    "                           |            |            |           |\n",
    "                           v            v            v           v\n",
    "                       [Lateral Convs - project to 256 channels]\n",
    "                           |            |            |           |\n",
    "                           v            v            v           v\n",
    "                       [MS-Deform-Attn Transformer Encoder]\n",
    "                           |            |            |           |\n",
    "                           v            v            v           v\n",
    "                       +--- FPN Upsample Path (top-down) ---+\n",
    "                           |            |            |           |\n",
    "                           v            v            v           v\n",
    "                      out_32        out_16       out_8       out_4\n",
    "                                                               |\n",
    "                                                               v\n",
    "                                                    [Pixel Features]\n",
    "                                                    (B, 256, H/4, W/4)\n",
    "```\n",
    "\n",
    "**Purpose:** Create rich, high-resolution features that the mask head can use for precise boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Component 2: Transformer Decoder with Masked Attention\n",
    "\n",
    "### Query-Based Approach\n",
    "\n",
    "Instead of per-pixel classification, Mask2Former uses **learnable queries** that each predict one mask:\n",
    "\n",
    "```\n",
    "Learnable Queries: Q = {q_1, q_2, ..., q_N}  (e.g., N=100)\n",
    "\n",
    "Each query learns to:\n",
    "  - Focus on one object/segment\n",
    "  - Predict its class\n",
    "  - Predict its mask\n",
    "```\n",
    "\n",
    "### Masked Cross-Attention\n",
    "\n",
    "The key innovation in Mask2Former is **masked attention** in the cross-attention layers:\n",
    "\n",
    "$$\\text{MaskedCrossAttn}(Q, K, V, M) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where $M$ is derived from the predicted mask from the **previous decoder layer**:\n",
    "\n",
    "$$M_{ij} = \\begin{cases} 0 & \\text{if } \\hat{m}_i[j] > 0.5 \\\\ -\\infty & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "**Effect:** Each query only attends to pixels within its predicted mask region!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masked_attention():\n",
    "    \"\"\"\n",
    "    Visualize how masked attention restricts cross-attention to predicted regions.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Create synthetic image with 3 regions\n",
    "    H, W = 64, 64\n",
    "    image = np.zeros((H, W, 3))\n",
    "    \n",
    "    # Region 1: Top-left circle (sky)\n",
    "    y, x = np.ogrid[:H, :W]\n",
    "    mask1 = ((x - 16)**2 + (y - 16)**2) < 12**2\n",
    "    image[mask1] = [0.5, 0.7, 1.0]  # Light blue\n",
    "    \n",
    "    # Region 2: Center rectangle (building)\n",
    "    mask2 = (x >= 24) & (x < 48) & (y >= 20) & (y < 50)\n",
    "    image[mask2] = [0.8, 0.6, 0.4]  # Brown\n",
    "    \n",
    "    # Region 3: Bottom (ground)\n",
    "    mask3 = (y >= 50) & ~mask2\n",
    "    image[mask3] = [0.3, 0.6, 0.3]  # Green\n",
    "    \n",
    "    # Row 1: Image and predicted masks\n",
    "    axes[0, 0].imshow(image)\n",
    "    axes[0, 0].set_title('Input Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(mask1.astype(float), cmap='Blues')\n",
    "    axes[0, 1].set_title('Query 1 Mask\\n(Sky)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(mask2.astype(float), cmap='Oranges')\n",
    "    axes[0, 2].set_title('Query 2 Mask\\n(Building)')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Row 2: Attention patterns\n",
    "    # Without masking - all pixels visible\n",
    "    attn_unmasked = np.random.rand(H, W)\n",
    "    axes[1, 0].imshow(attn_unmasked, cmap='hot')\n",
    "    axes[1, 0].set_title('Standard Cross-Attention\\n(Query attends to ALL pixels)')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # With masking - Query 1 only sees sky region\n",
    "    attn_masked1 = np.random.rand(H, W) * mask1.astype(float)\n",
    "    axes[1, 1].imshow(attn_masked1, cmap='hot')\n",
    "    axes[1, 1].set_title('Masked Attention (Query 1)\\n(Only attends to sky pixels)')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # With masking - Query 2 only sees building region\n",
    "    attn_masked2 = np.random.rand(H, W) * mask2.astype(float)\n",
    "    axes[1, 2].imshow(attn_masked2, cmap='hot')\n",
    "    axes[1, 2].set_title('Masked Attention (Query 2)\\n(Only attends to building pixels)')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.suptitle('Mask2Former: Masked Cross-Attention Visualization', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_masked_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Why Masked Attention Improves Convergence\n",
    "\n",
    "### Problem with Standard Cross-Attention:\n",
    "- Query attends to **all pixels** equally at first\n",
    "- Gradients are distributed across the entire image\n",
    "- Learning signal is diluted\n",
    "\n",
    "### Solution with Masked Attention:\n",
    "1. **Focused Learning:** Each query only receives gradients from its relevant region\n",
    "2. **Iterative Refinement:** Masks improve over decoder layers, attention becomes more focused\n",
    "3. **Prevents Interference:** Different queries don't compete for the same pixels\n",
    "\n",
    "```\n",
    "Decoder Layer 1:  Rough masks → Wide attention regions\n",
    "                       ↓\n",
    "Decoder Layer 2:  Better masks → Tighter attention\n",
    "                       ↓\n",
    "Decoder Layer 3:  Refined masks → Precise attention\n",
    "                       ↓\n",
    "         ...           ↓\n",
    "Decoder Layer 9:  Final masks → Highly focused attention\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Complete Mask2Former Decoder Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask2former_decoder_flow = \"\"\"\n",
    "Mask2Former Transformer Decoder (9 layers, iterative refinement)\n",
    "================================================================\n",
    "\n",
    "Inputs:\n",
    "  - queries: (B, N, 256) - N learnable query embeddings (N=100)\n",
    "  - pixel_features: (B, 256, H/4, W/4) - from Pixel Decoder\n",
    "  - multi_scale_features: {H/8, H/16, H/32} - from Pixel Decoder\n",
    "\n",
    "For each decoder layer l in [1, 2, ..., 9]:\n",
    "    \n",
    "    1. Masked Cross-Attention\n",
    "       -------------------------\n",
    "       if l > 1:\n",
    "           mask_l = sigmoid(mask_predictions[l-1])  # From previous layer\n",
    "           attention_mask = (mask_l > 0.5)          # Binary mask\n",
    "       else:\n",
    "           attention_mask = None  # Layer 1: no masking\n",
    "       \n",
    "       queries = MaskedCrossAttn(\n",
    "           query=queries,\n",
    "           key=pixel_features,\n",
    "           value=pixel_features,\n",
    "           mask=attention_mask\n",
    "       )\n",
    "    \n",
    "    2. Self-Attention (among queries)\n",
    "       -------------------------\n",
    "       queries = SelfAttn(queries)  # Queries interact with each other\n",
    "    \n",
    "    3. FFN\n",
    "       -------------------------\n",
    "       queries = FFN(queries)\n",
    "    \n",
    "    4. Prediction Heads\n",
    "       -------------------------\n",
    "       class_logits[l] = ClassHead(queries)     # (B, N, num_classes+1)\n",
    "       mask_logits[l] = MaskHead(queries, pixel_features)  # (B, N, H, W)\n",
    "\n",
    "Output:\n",
    "  - Final predictions from last layer\n",
    "  - Auxiliary predictions from intermediate layers (for training)\n",
    "\"\"\"\n",
    "print(mask2former_decoder_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Task-Agnostic Queries\n",
    "\n",
    "One powerful aspect of Mask2Former is that **the same queries work for all segmentation tasks**:\n",
    "\n",
    "| Task | Query Interpretation | Post-Processing |\n",
    "|------|---------------------|----------------|\n",
    "| **Semantic Segmentation** | Each query = one semantic class | Merge masks by class ID |\n",
    "| **Instance Segmentation** | Each query = one object instance | Keep masks separate |\n",
    "| **Panoptic Segmentation** | Queries = stuff + things | Combine semantic + instance |\n",
    "\n",
    "The only difference is in **post-processing**, not the model architecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: Putting It All Together - Our Pipeline\n",
    "\n",
    "## 6.1 Complete Architecture Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_diagram = r\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    DINOv3 + ViT-Adapter + Mask2Former Pipeline                    ║\n",
    "╠═══════════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                                   ║\n",
    "║  Input Image (720×720×3)                                                          ║\n",
    "║        │                                                                          ║\n",
    "║        │                                                                          ║\n",
    "║        ▼                                                                          ║\n",
    "║  ┌─────────────────────────────────────────────────────────────────────────────┐  ║\n",
    "║  │                        FROZEN: DINOv3-ViT-L/16 (~1B params)                 │  ║\n",
    "║  │  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐              │  ║\n",
    "║  │  │ Layer 4  │ →  │ Layer 11 │ →  │ Layer 17 │ →  │ Layer 23 │              │  ║\n",
    "║  │  │ (patch)  │    │ (patch)  │    │ (patch)  │    │ (patch)  │              │  ║\n",
    "║  │  └────┬─────┘    └────┬─────┘    └────┬─────┘    └────┬─────┘              │  ║\n",
    "║  └───────│───────────────│───────────────│───────────────│────────────────────┘  ║\n",
    "║          │               │               │               │                        ║\n",
    "║          │               │               │               │                        ║\n",
    "║  ┌───────▼───────────────▼───────────────▼───────────────▼────────────────────┐  ║\n",
    "║  │                    TRAINABLE: ViT-Adapter (~50M params)                    │  ║\n",
    "║  │                                                                            │  ║\n",
    "║  │  ┌──────────────┐     ┌──────────────────────────────────────────────────┐ │  ║\n",
    "║  │  │   Spatial    │     │           Interaction Blocks (×4)                │ │  ║\n",
    "║  │  │    Prior     │────▶│  MS-Deformable Attention + ConvFFN               │ │  ║\n",
    "║  │  │   Module     │     │  (Fuses CNN spatial priors with ViT semantics)   │ │  ║\n",
    "║  │  │  (CNN Stem)  │     └──────────────────────────────────────────────────┘ │  ║\n",
    "║  │  └──────────────┘                                                          │  ║\n",
    "║  │                                                                            │  ║\n",
    "║  │  Output: Multi-scale features {f1: H/4, f2: H/8, f3: H/16, f4: H/32}       │  ║\n",
    "║  └────────────────────────────────────────────────────────────────────────────┘  ║\n",
    "║          │                                                                        ║\n",
    "║          │                                                                        ║\n",
    "║          ▼                                                                        ║\n",
    "║  ┌────────────────────────────────────────────────────────────────────────────┐  ║\n",
    "║  │                    TRAINABLE: Mask2Former (~40M params)                    │  ║\n",
    "║  │                                                                            │  ║\n",
    "║  │  ┌─────────────────────┐    ┌────────────────────────────────────────────┐ │  ║\n",
    "║  │  │    Pixel Decoder    │    │         Transformer Decoder (×9)          │ │  ║\n",
    "║  │  │  (FPN + MS-Deform)  │───▶│  Learnable Queries (100)                   │ │  ║\n",
    "║  │  │                     │    │  + Masked Cross-Attention                  │ │  ║\n",
    "║  │  │  → Pixel Features   │    │  → Class Logits + Mask Logits              │ │  ║\n",
    "║  │  └─────────────────────┘    └────────────────────────────────────────────┘ │  ║\n",
    "║  └────────────────────────────────────────────────────────────────────────────┘  ║\n",
    "║          │                                                                        ║\n",
    "║          ▼                                                                        ║\n",
    "║  ┌────────────────────────────────────────────────────────────────────────────┐  ║\n",
    "║  │  Output: Semantic Segmentation (7 classes for LoveDA)                      │  ║\n",
    "║  │         - Class predictions: (B, 100, 7)                                   │  ║\n",
    "║  │         - Mask predictions:  (B, 100, H, W)                                │  ║\n",
    "║  │         → Post-process to (B, 7, H, W) semantic map                        │  ║\n",
    "║  └────────────────────────────────────────────────────────────────────────────┘  ║\n",
    "║                                                                                   ║\n",
    "╚═══════════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "print(architecture_diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Key File Locations in Our Codebase\n",
    "\n",
    "| Component | File | Key Classes/Functions |\n",
    "|-----------|------|----------------------|\n",
    "| **DINOv3 Wrapper** | `dinov3_mask2former_integration.py:85-155` | `DINOv3CompatibilityWrapper` |\n",
    "| **ViT-Adapter** | `models/backbone/dinov3_adapter.py:305-484` | `DINOv3_Adapter` |\n",
    "| **Spatial Prior Module** | `models/backbone/dinov3_adapter.py:234-302` | `SpatialPriorModule` |\n",
    "| **Interaction Block** | `models/backbone/dinov3_adapter.py:159-231` | `InteractionBlockWithCls` |\n",
    "| **MS-Deformable Attention** | `models/utils/ms_deform_attn.py:99-214` | `MSDeformAttn` |\n",
    "| **Model Creation** | `dinov3_mask2former_integration.py:248-444` | `create_dinov3_mask2former()` |\n",
    "| **Training** | `train_hydra.py:1-391` | `SegmentationLightningModule` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Running Inference (Quick Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates how to load our model and run inference\n",
    "# (Requires HuggingFace token for DINOv3 access)\n",
    "\n",
    "inference_code = \"\"\"\n",
    "# To run inference with our trained model:\n",
    "\n",
    "from dinov3_mask2former_integration import create_dinov3_mask2former\n",
    "import torch\n",
    "\n",
    "# 1. Create model\n",
    "model, processor, _ = create_dinov3_mask2former(\n",
    "    dinov3_model_name=\"facebook/dinov3-vitl16-pretrain-sat493m\",\n",
    "    num_classes=7  # LoveDA classes\n",
    ")\n",
    "\n",
    "# 2. Load trained checkpoint\n",
    "checkpoint = torch.load('runs/your_run/checkpoints/best.ckpt')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# 3. Prepare image\n",
    "from PIL import Image\n",
    "image = Image.open('your_image.png').convert('RGB')\n",
    "inputs = processor(images=image, return_tensors='pt')\n",
    "\n",
    "# 4. Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 5. Post-process to get semantic segmentation map\n",
    "semantic_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[(image.height, image.width)]\n",
    ")[0]\n",
    "\n",
    "print(f\"Segmentation map shape: {semantic_map.shape}\")\n",
    "\"\"\"\n",
    "print(inference_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: Key Takeaways\n",
    "\n",
    "## 1. Vision Transformer (ViT)\n",
    "- Processes images as sequences of patches\n",
    "- Global attention from layer 1 (no local inductive bias)\n",
    "- Attention maps can be visualized via rollout or per-layer analysis\n",
    "\n",
    "## 2. Position Encoding Interpolation\n",
    "- 2D bicubic interpolation enables variable input resolutions\n",
    "- Preserves spatial relationships learned during pre-training\n",
    "- Essential for using ViT at resolutions different from pre-training\n",
    "\n",
    "## 3. ViT-Adapter\n",
    "- Bridges ViT to dense prediction via multi-scale features\n",
    "- Spatial Prior Module adds CNN-like local inductive bias\n",
    "- Interaction blocks fuse SPM and ViT features via deformable attention\n",
    "\n",
    "## 4. MS-Deformable Attention\n",
    "- O(N) complexity vs O(N²) for standard attention\n",
    "- Sparse sampling at learned offset positions\n",
    "- Multi-scale: samples from features at different resolutions\n",
    "\n",
    "## 5. Mask2Former\n",
    "- Query-based universal segmentation\n",
    "- Masked attention restricts cross-attention to predicted regions\n",
    "- Iterative refinement across decoder layers\n",
    "- Task-agnostic: same architecture for semantic/instance/panoptic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Further Reading & Resources\n",
    "\n",
    "## Papers\n",
    "- **ViT:** [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)\n",
    "- **DINOv2:** [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)\n",
    "- **ViT-Adapter:** [Vision Transformer Adapter for Dense Predictions](https://arxiv.org/abs/2205.08534)\n",
    "- **Deformable DETR:** [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)\n",
    "- **Mask2Former:** [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527)\n",
    "\n",
    "## Codebase Documentation\n",
    "- `docs/ARCHITECTURE.md` - Detailed architecture explanation\n",
    "- `docs/TRAINING.md` - Training guide\n",
    "- `docs/CONFIGURATION.md` - Hydra config reference\n",
    "\n",
    "## Code Deep Dives\n",
    "- Attention visualization: This notebook Part 1\n",
    "- Position interpolation: `models/backbone/dinov3_adapter.py:387-396`\n",
    "- MS-Deformable Attention: `models/utils/ms_deform_attn.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
